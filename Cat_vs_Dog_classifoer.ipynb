{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOctCkjLBwmxUg1qC4Hqhlh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anushamahajan5/Dog_cat_classifier/blob/main/Cat_vs_Dog_classifoer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/udacity/cd0281-Introduction-to-Neural-Networks-with-PyTorch/tree/master/gradient-descent"
      ],
      "metadata": {
        "id": "KmyqtvFhQAFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Readme:   https://www.youtube.com/watch?time_continue=36&v=d_NhvI1yEf0&embeds_referring_euri=https%3A%2F%2Flearn.udacity.com%2F&embeds_referring_origin=https%3A%2F%2Flearn.udacity.com&source_ve_path=Mjg2NjY&feature=emb_logo"
      ],
      "metadata": {
        "id": "GY48FNClUtaj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Image Data\n",
        "\n",
        "So far we've been working with fairly artificial datasets that you wouldn't typically be using in real projects. Instead, you'll likely be dealing with full-sized images like you'd get from smart phone cameras. In this notebook, we'll look at how to load images and use them to train neural networks.\n",
        "\n",
        "We'll be using a [dataset of cat and dog photos](https://www.kaggle.com/c/dogs-vs-cats) available from Kaggle. Here are a couple example images:\n",
        "\n",
        "<img src='data:image/jpeg;base64,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'>\n",
        "\n",
        "We'll use this dataset to train a neural network that can differentiate between cats and dogs. These days it doesn't seem like a big accomplishment, but five years ago it was a serious challenge for computer vision systems."
      ],
      "metadata": {
        "id": "XBm8h_y7R75b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6da0G7fNsdi",
        "outputId": "1d94e4bc-782d-4707-8a09-14f16976cf04"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the Google drive so you can store your kaggle API credentials for future use"
      ],
      "metadata": {
        "id": "DQlgGDYwOoo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C8f2PTcMpU1",
        "outputId": "7aea87ab-7b86-4333-bbf0-60f9c3d1c162"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a directory for kaggle at the temporary instance location on Colab drive.\n",
        "\n",
        "Download your kaggle API key (.json file). You can do this by going to your kaggle account page and clicking 'Create new API token' under the API section."
      ],
      "metadata": {
        "id": "VkKJ6LSMPBYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "qAUAFJOBPEPG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the json file to Google Drive and then copy to the temporary location."
      ],
      "metadata": {
        "id": "Ziz6W44tPQkG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "qddC5esFPSOS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "! kaggle competitions download 'Asirra: Cats vs Dogs Object Detection Dataset'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ts_V1Z1lQmMd",
        "outputId": "d8cf523c-397a-4249-f4aa-c4d2f72b2da9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "403 - Forbidden - Permission 'competitions.participate' was denied\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uv8v9Y0Aeur"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import helper"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The easiest way to load image data is with `datasets.ImageFolder` from `torchvision` ([documentation](http://pytorch.org/docs/master/torchvision/datasets.html#imagefolder)). In general you'll use `ImageFolder` like so:\n",
        "\n",
        "```python\n",
        "dataset = datasets.ImageFolder('path/to/data', transform=transform)\n",
        "```\n",
        "\n",
        "where `'path/to/data'` is the file path to the data directory and `transform` is a sequence of processing steps built with the [`transforms`](http://pytorch.org/docs/master/torchvision/transforms.html) module from `torchvision`. ImageFolder expects the files and directories to be constructed like so:\n",
        "```\n",
        "root/dog/xxx.png\n",
        "root/dog/xxy.png\n",
        "root/dog/xxz.png\n",
        "\n",
        "root/cat/123.png\n",
        "root/cat/nsdf3.png\n",
        "root/cat/asd932_.png\n",
        "```\n",
        "\n",
        "where each class has it's own directory (`cat` and `dog`) for the images. The images are then labeled with the class taken from the directory name. So here, the image `123.png` would be loaded with the class label `cat`. You can download the dataset already structured like this [from here](https://s3.amazonaws.com/content.udacity-data.com/nd089/Cat_Dog_data.zip). I've also split it into a training set and test set.\n",
        "\n",
        "### Transforms\n",
        "When you load in the data with `ImageFolder`, you'll need to define some transforms. For example, the images are different sizes but we'll need them to all be the same size for training. You can either resize them with `transforms.Resize()` or crop with `transforms.CenterCrop()`, `transforms.RandomResizedCrop()`, etc. We'll also need to convert the images to PyTorch tensors with `transforms.ToTensor()`. Typically you'll combine these transforms into a pipeline with `transforms.Compose()`, which accepts a list of transforms and runs them in sequence. It looks something like this to scale, then crop, then convert to a tensor:\n",
        "\n",
        "```python\n",
        "transform = transforms.Compose([transforms.Resize(255),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor()])\n",
        "\n",
        "```\n",
        "\n",
        "There are plenty of transforms available, I'll cover more in a bit and you can read through the [documentation](http://pytorch.org/docs/master/torchvision/transforms.html).\n",
        "\n",
        "### Data Loaders\n",
        "\n",
        "With the `ImageFolder` loaded, you have to pass it to a [`DataLoader`](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader). The `DataLoader` takes a dataset (such as you would get from `ImageFolder`) and returns batches of images and the corresponding labels. You can set various parameters like the batch size and if the data is shuffled after each epoch.\n",
        "\n",
        "```python\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "```\n",
        "\n",
        "Here `dataloader` is a [generator](https://jeffknupp.com/blog/2013/04/07/improve-your-python-yield-and-generators-explained/). To get data out of it, you need to loop through it or convert it to an iterator and call `next()`.\n",
        "\n",
        "```python\n",
        "# Looping through it, get a batch on each loop\n",
        "for images, labels in dataloader:\n",
        "    pass\n",
        "\n",
        "# Get one batch\n",
        "images, labels = next(iter(dataloader))\n",
        "```\n",
        "\n",
        ">**Exercise:** Load images from the `Cat_Dog_data/train` folder, define a few transforms, then build the dataloader."
      ],
      "metadata": {
        "id": "CX8AlF_8SIx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'Cat_Dog_data/train'\n",
        "\n",
        "transform = transforms.Compose([transforms.Resize(255),\n",
        "                                transforms.CenterCrop(224),\n",
        "                                transforms.ToTensor()])\n",
        "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "vsY2mF5oSYbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to test your data loader\n",
        "images, labels = next(iter(dataloader))\n",
        "helper.imshow(images[0], normalize=False)"
      ],
      "metadata": {
        "id": "ph-mOd4aSdU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you loaded the data correctly, you should see something like this (your image will be different):\n",
        "\n",
        "<img src='assets/cat_cropped.png' width=244>"
      ],
      "metadata": {
        "id": "6Ox3cBGPSgPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation\n",
        "\n",
        "A common strategy for training neural networks is to introduce randomness in the input data itself. For example, you can randomly rotate, mirror, scale, and/or crop your images during training. This will help your network generalize as it's seeing the same images but in different locations, with different sizes, in different orientations, etc.\n",
        "\n",
        "To randomly rotate, scale and crop, then flip your images you would define your transforms like this:\n",
        "\n",
        "```python\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize([0.5, 0.5, 0.5],\n",
        "                                                            [0.5, 0.5, 0.5])])\n",
        "```\n",
        "\n",
        "You'll also typically want to normalize images with `transforms.Normalize`. You pass in a list of means and list of standard deviations, then the color channels are normalized like so\n",
        "\n",
        "```input[channel] = (input[channel] - mean[channel]) / std[channel]```\n",
        "\n",
        "Subtracting `mean` centers the data around zero and dividing by `std` squishes the values to be between -1 and 1. Normalizing helps keep the network weights near zero which in turn makes backpropagation more stable. Without normalization, networks will tend to fail to learn.\n",
        "\n",
        "You can find a list of all [the available transforms here](http://pytorch.org/docs/0.3.0/torchvision/transforms.html). When you're testing however, you'll want to use images that aren't altered other than normalizing. So, for validation/test images, you'll typically just resize and crop.\n",
        "\n",
        ">**Exercise:** Define transforms for training data and testing data below. Leave off normalization for now."
      ],
      "metadata": {
        "id": "xIcCDzBzUXHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'Cat_Dog_data'\n",
        "\n",
        "# TODO: Define transforms for the training data and testing data\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor()])\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.Resize(255),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor()])\n",
        "\n",
        "\n",
        "# Pass transforms in here, then run the next cell to see how the transforms look\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=32)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=32)"
      ],
      "metadata": {
        "id": "-apJuBBAUQZG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "fc23fc07-7a11-49ac-ab15-90a650538442"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Cat_Dog_data/train'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-bcf09b117736>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Pass transforms in here, then run the next cell to see how the transforms look\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Cat_Dog_data/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change this to the trainloader or testloader\n",
        "data_iter = iter(testloader)\n",
        "\n",
        "images, labels = next(data_iter)\n",
        "fig, axes = plt.subplots(figsize=(10,4), ncols=4)\n",
        "for ii in range(4):\n",
        "    ax = axes[ii]\n",
        "    helper.imshow(images[ii], ax=ax, normalize=False)"
      ],
      "metadata": {
        "id": "PhBrJcr0UZDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your transformed images should look something like this.\n",
        "\n",
        "<center>Training examples:</center>\n",
        "<img src='assets/train_examples.png' width=500px>\n",
        "\n",
        "<center>Testing examples:</center>\n",
        "<img src='assets/test_examples.png' width=500px>"
      ],
      "metadata": {
        "id": "s55iTfcAUdRB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point you should be able to load data for training and testing. Now, you should try building a network that can classify cats vs dogs. This is quite a bit more complicated than before with the MNIST and Fashion-MNIST datasets. To be honest, you probably won't get it to work with a fully-connected network, no matter how deep. These images have three color channels and at a higher resolution (so far you've seen 28x28 images which are tiny).\n",
        "\n",
        "In the next part, I'll show you how to use a pre-trained network to build a model that can actually solve this problem."
      ],
      "metadata": {
        "id": "WYC4aL-iUls5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the solution: https://www.youtube.com/watch?time_continue=91&v=4n6T93hKRD4&embeds_referring_euri=https%3A%2F%2Flearn.udacity.com%2F&embeds_referring_origin=https%3A%2F%2Flearn.udacity.com&source_ve_path=Mjg2NjY&feature=emb_logo"
      ],
      "metadata": {
        "id": "kzy9zbeE1sri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n",
        "\n",
        "In this notebook, you'll learn how to use pre-trained networks to solved challenging problems in computer vision. Specifically, you'll use networks trained on [ImageNet](http://www.image-net.org/) [available from torchvision](http://pytorch.org/docs/0.3.0/torchvision/models.html).\n",
        "\n",
        "ImageNet is a massive dataset with over 1 million labeled images in 1000 categories. It's used to train deep neural networks using an architecture called convolutional layers. I'm not going to get into the details of convolutional networks here, but if you want to learn more about them, please [watch this](https://www.youtube.com/watch?v=2-Ol7ZB0MmU).\n",
        "\n",
        "Once trained, these models work astonishingly well as feature detectors for images they weren't trained on. Using a pre-trained network on images not in the training set is called transfer learning. Here we'll use transfer learning to train a network that can classify our cat and dog photos with near perfect accuracy.\n",
        "\n",
        "With `torchvision.models` you can download these pre-trained networks and use them in your applications. We'll include `models` in our imports now."
      ],
      "metadata": {
        "id": "LiAB1O8D4WQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models"
      ],
      "metadata": {
        "id": "MQJ6j3QWUhtd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the pretrained models require the input to be 224x224 images. Also, we'll need to match the normalization used when the models were trained. Each color channel was normalized separately, the means are `[0.485, 0.456, 0.406]` and the standard deviations are `[0.229, 0.224, 0.225]`."
      ],
      "metadata": {
        "id": "lFYb6Wh24aXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = 'Cat_Dog_data'\n",
        "\n",
        "# TODO: Define transforms for the training data and testing data\n",
        "train_transforms = transforms.Compose([transforms.RandomRotation(30),\n",
        "                                       transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                            [0.229, 0.224, 0.225])])\n",
        "\n",
        "test_transforms = transforms.Compose([transforms.Resize(255),\n",
        "                                      transforms.CenterCrop(224),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                           [0.229, 0.224, 0.225])])\n",
        "\n",
        "# Pass transforms in here, then run the next cell to see how the transforms look\n",
        "train_data = datasets.ImageFolder(data_dir + '/train', transform=train_transforms)\n",
        "test_data = datasets.ImageFolder(data_dir + '/test', transform=test_transforms)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=64)"
      ],
      "metadata": {
        "id": "eVDucxJm4fg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can load in a model such as [DenseNet](http://pytorch.org/docs/0.3.0/torchvision/models.html#id5). Let's print out the model architecture so we can see what's going on."
      ],
      "metadata": {
        "id": "nrqC09if43wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.densenet121(pretrained=True)\n",
        "model"
      ],
      "metadata": {
        "id": "NwwoubPv47Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model is built out of two main parts, the features and the classifier. The features part is a stack of convolutional layers and overall works as a feature detector that can be fed into a classifier. The classifier part is a single fully-connected layer `(classifier): Linear(in_features=1024, out_features=1000)`. This layer was trained on the ImageNet dataset, so it won't work for our specific problem. That means we need to replace the classifier, but the features will work perfectly on their own. In general, I think about pre-trained networks as amazingly good feature detectors that can be used as the input for simple feed-forward classifiers."
      ],
      "metadata": {
        "id": "h1G5eiUL4-AT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze parameters so we don't backprop through them\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "from collections import OrderedDict\n",
        "classifier = nn.Sequential(OrderedDict([\n",
        "                          ('fc1', nn.Linear(1024, 500)),\n",
        "                          ('relu', nn.ReLU()),\n",
        "                          ('fc2', nn.Linear(500, 2)),\n",
        "                          ('output', nn.LogSoftmax(dim=1))\n",
        "                          ]))\n",
        "\n",
        "model.classifier = classifier"
      ],
      "metadata": {
        "id": "0CJipRxj5G5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our model built, we need to train the classifier. However, now we're using a **really deep** neural network. If you try to train this on a CPU like normal, it will take a long, long time. Instead, we're going to use the GPU to do the calculations. The linear algebra computations are done in parallel on the GPU leading to 100x increased training speeds. It's also possible to train on multiple GPUs, further decreasing training time.\n",
        "\n",
        "PyTorch, along with pretty much every other deep learning framework, uses [CUDA](https://developer.nvidia.com/cuda-zone) to efficiently compute the forward and backwards passes on the GPU. In PyTorch, you move your model parameters and other tensors to the GPU memory using `model.to('cuda')`. You can move them back from the GPU with `model.to('cpu')` which you'll commonly do when you need to operate on the network output outside of PyTorch. As a demonstration of the increased speed, I'll compare how long it takes to perform a forward and backward pass with and without a GPU."
      ],
      "metadata": {
        "id": "JYIjV0id5LLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "ULbQLRkZ5V8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try to replace with just ['cuda'] if you are using GPU\n",
        "\n",
        "for device in ['cpu', 'cuda']:\n",
        "\n",
        "    criterion = nn.NLLLoss()\n",
        "    # Only train the classifier parameters, feature parameters are frozen\n",
        "    optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for ii, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "        # Move input and label tensors to the GPU\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        outputs = model.forward(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if ii==3:\n",
        "            break\n",
        "\n",
        "    print(f\"Device = {device}; Time per batch: {(time.time() - start)/3:.3f} seconds\")"
      ],
      "metadata": {
        "id": "KwWcFdNz5ax8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can write device agnostic code which will automatically use CUDA if it's enabled like so:\n",
        "```python\n",
        "# at beginning of the script\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "...\n",
        "\n",
        "# then whenever you get a new Tensor or Module\n",
        "# this won't copy if they are already on the desired device\n",
        "input = data.to(device)\n",
        "model = MyModule(...).to(device)\n",
        "```\n",
        "\n",
        "From here, I'll let you finish training the model. The process is the same as before except now your model is much more powerful. You should get better than 95% accuracy easily.\n",
        "\n",
        ">**Exercise:** Train a pretrained models to classify the cat and dog images. Continue with the DenseNet model, or try ResNet, it's also a good model to try out first. Make sure you are only training the classifier and the parameters for the features part are frozen."
      ],
      "metadata": {
        "id": "VXvhpT-65Sqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use GPU if it's available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = models.densenet121(pretrained=True)\n",
        "\n",
        "# Freeze parameters so we don't backprop through them\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model.classifier = nn.Sequential(nn.Linear(1024, 256),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Dropout(0.2),\n",
        "                                 nn.Linear(256, 2),\n",
        "                                 nn.LogSoftmax(dim=1))\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "# Only train the classifier parameters, feature parameters are frozen\n",
        "optimizer = optim.Adam(model.classifier.parameters(), lr=0.003)\n",
        "\n",
        "model.to(device);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_QB5NBN5ifb",
        "outputId": "71237b83-6347-4dcf-cf57-8f19cec14beb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
            "100%|██████████| 30.8M/30.8M [00:00<00:00, 68.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "steps = 0\n",
        "running_loss = 0\n",
        "print_every = 5\n",
        "for epoch in range(epochs):\n",
        "    for inputs, labels in trainloader:\n",
        "        steps += 1\n",
        "        # Move input and label tensors to the default device\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logps = model.forward(inputs)\n",
        "        loss = criterion(logps, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        if steps % print_every == 0:\n",
        "            test_loss = 0\n",
        "            accuracy = 0\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in testloader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    logps = model.forward(inputs)\n",
        "                    batch_loss = criterion(logps, labels)\n",
        "\n",
        "                    test_loss += batch_loss.item()\n",
        "\n",
        "                    # Calculate accuracy\n",
        "                    ps = torch.exp(logps)\n",
        "                    top_p, top_class = ps.topk(1, dim=1)\n",
        "                    equals = top_class == labels.view(*top_class.shape)\n",
        "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}.. \"\n",
        "                  f\"Train loss: {running_loss/print_every:.3f}.. \"\n",
        "                  f\"Test loss: {test_loss/len(testloader):.3f}.. \"\n",
        "                  f\"Test accuracy: {accuracy/len(testloader):.3f}\")\n",
        "            running_loss = 0\n",
        "            model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "mSzfFmvR5s-y",
        "outputId": "1af8f49e-87a1-4bb1-c8c8-997ef71404da"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainloader' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-8aaa6a92c6cd>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint_every\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Move input and label tensors to the default device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainloader' is not defined"
          ]
        }
      ]
    }
  ]
}